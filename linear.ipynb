{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUh0qKy/mmFtOrOKGLkJfs",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KJOELJOYSON2427/Linear_Regression_Model/blob/main/linear.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wEupEf_Cc9Wv"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import abstractclassmethod\n",
        "class Linear_Regression():\n",
        "  @abstractclassmethod\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  @abstractclassmethod\n",
        "  def fit(self):\n",
        "    pass\n",
        "  @abstractclassmethod\n",
        "  def update_weights(self):\n",
        "    pass\n",
        "  @abstractclassmethod\n",
        "  def predict(self):\n",
        "    pass"
      ],
      "metadata": {
        "id": "G569pM64dIc4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegression(Linear_Regression):\n",
        "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_iterations = n_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Initialize weights and bias\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        # Gradient Descent\n",
        "        for _ in range(self.n_iterations):\n",
        "            # Approximate y with linear combination of weights and x, plus bias\n",
        "            y_predicted = self.predict(X)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = (1/n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1/n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            # Update weights and bias\n",
        "            self.update_weights(dw, db)\n",
        "\n",
        "    def update_weights(self, dw, db):\n",
        "        # Update weights and bias using gradients and learning rate\n",
        "        self.weights -= self.learning_rate * dw\n",
        "        self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict(self, X):\n",
        "        # Calculate predictions\n",
        "        y_predicted = np.dot(X, self.weights) + self.bias\n",
        "        print(y_predicted)\n",
        "        return y_predicted\n",
        "\n"
      ],
      "metadata": {
        "id": "EbQ_59TGe46P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7afdac1"
      },
      "source": [
        "# Linear Regression Implementation\n",
        "\n",
        "This notebook contains a Python implementation of Linear Regression using gradient descent.\n",
        "\n",
        "## Class: `Linear_Regression` (Abstract Base Class)\n",
        "This class serves as an abstract base class defining the required methods for a linear regression model:\n",
        "- `__init__()`: Constructor\n",
        "- `fit()`: Method for training the model\n",
        "- `update_weights()`: Method for updating model weights\n",
        "- `predict()`: Method for making predictions\n",
        "\n",
        "## Class: `LinearRegression`\n",
        "This class inherits from `Linear_Regression` and provides a concrete implementation of the linear regression algorithm using gradient descent.\n",
        "\n",
        "### Parameters:\n",
        "- `learning_rate`: The step size for updating weights during gradient descent.\n",
        "- `n_iterations`: The number of iterations for the gradient descent algorithm.\n",
        "\n",
        "### Methods:\n",
        "- `__init__(self, learning_rate=0.01, n_iterations=1000)`: Initializes the learning rate, number of iterations, and sets initial weights and bias to None.\n",
        "- `fit(self, X, y)`: Trains the model using the provided input features `X` and target variable `y`. It initializes weights and bias, then iteratively updates them using gradient descent for a specified number of iterations.\n",
        "- `update_weights(self, dw, db)`: Updates the model's weights and bias based on the calculated gradients (`dw` and `db`) and the learning rate.\n",
        "- `predict(self, X)`: Predicts the output for the given input features `X` using the learned weights and bias.\n",
        "\n",
        "## Gradient Descent Details:\n",
        "The `fit` method uses gradient descent to minimize the mean squared error. The gradients for the weights (`dw`) and bias (`db`) are calculated as follows:\n",
        "\n",
        "$dw = \\frac{1}{n\\_samples} \\cdot X^T \\cdot (y\\_{predicted} - y)$\n",
        "\n",
        "$db = \\frac{1}{n\\_samples} \\cdot \\sum (y\\_{predicted} - y)$\n",
        "\n",
        "Where:\n",
        "- $n\\_samples$ is the number of training samples.\n",
        "- $X$ is the input feature matrix.\n",
        "- $X^T$ is the transpose of the input feature matrix.\n",
        "- $y\\_{predicted}$ is the vector of predicted outputs.\n",
        "- $y$ is the vector of actual outputs.\n",
        "\n",
        "The weights and bias are updated in each iteration using the following rules:\n",
        "\n",
        "$weights = weights - learning\\_rate \\cdot dw$\n",
        "\n",
        "$bias = bias - learning\\_rate \\cdot db$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cf9d758",
        "outputId": "dcbffc93-5eec-4987-c7c7-82d07269d126"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sample data\n",
        "X = np.array([[1, 2],\n",
        "              [3, 4],\n",
        "              [5, 6]]) # 3 samples, 2 features\n",
        "\n",
        "y = np.array([7, 8, 9]) # Actual outputs\n",
        "\n",
        "# Example predicted outputs (could be from an initial guess or previous iteration)\n",
        "y_predicted = np.array([6.5, 8.2, 9.8])\n",
        "\n",
        "# Calculate the error\n",
        "error = y_predicted - y\n",
        "\n",
        "# Calculate the dot product of X.T and the error\n",
        "dot_product_result = np.dot(X.T, error)\n",
        "\n",
        "print(\"X (input features):\\n\", X)\n",
        "print(\"\\nX.T (transpose of X):\\n\", X.T)\n",
        "print(\"\\ny (actual outputs):\\n\", y)\n",
        "print(\"\\ny_predicted (predicted outputs):\\n\", y_predicted)\n",
        "print(\"\\nerror (y_predicted - y):\\n\", error)\n",
        "print(\"\\nnp.dot(X.T, error) (dot product of X.T and error):\\n\", dot_product_result)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X (input features):\n",
            " [[1 2]\n",
            " [3 4]\n",
            " [5 6]]\n",
            "\n",
            "X.T (transpose of X):\n",
            " [[1 3 5]\n",
            " [2 4 6]]\n",
            "\n",
            "y (actual outputs):\n",
            " [7 8 9]\n",
            "\n",
            "y_predicted (predicted outputs):\n",
            " [6.5 8.2 9.8]\n",
            "\n",
            "error (y_predicted - y):\n",
            " [-0.5  0.2  0.8]\n",
            "\n",
            "np.dot(X.T, error) (dot product of X.T and error):\n",
            " [4.1 4.6]\n"
          ]
        }
      ]
    }
  ]
}